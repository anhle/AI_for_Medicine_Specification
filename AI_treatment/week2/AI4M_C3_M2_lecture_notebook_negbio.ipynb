{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "AI4M_C3_M2_lecture_notebook_negbio.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anhle/AI_for_Medicine_Specification/blob/master/AI_treatment/week2/AI4M_C3_M2_lecture_notebook_negbio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svhoSEu7zgDI",
        "colab_type": "text"
      },
      "source": [
        "## AI for Medicine Course 3 Week 1 lecture notebook\n",
        "## Using BioC format and the NegBio Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TPqhIBRzgDJ",
        "colab_type": "text"
      },
      "source": [
        "Welcome to this lecture notebook! You'll be exploring some of the uses of the `NegBio` library, a tool for biomedical text mining, which you will use in the graded assignment at the end of the week.\n",
        "\n",
        "You'll be using the same dataset as in the assignment, so this is a good opportunity to become more familiar with it. \n",
        "- This dataset consists of 1,000 X-ray reports that have been manually labeled by a board-certified radiologist.\n",
        "- The reports indicate the presence or absence of several different pathologies. \n",
        "- You'll also have access to the extracted \"Report Impression\" section of each report, which is the summary provided for each X-ray. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pIvcU8IzgDK",
        "colab_type": "text"
      },
      "source": [
        "### Import Pandas and Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bJCoRZ6FzgDK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data from file\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/anhle/AI_for_Medicine_Specification/master/AI_treatment/week2/stanford_report_test.csv\")\n",
        "\n",
        "# Check the num of rows, columns\n",
        "print(f\"dataset has shape: {df.shape}\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYJ_gDV3zgDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a better view of the report impression column\n",
        "for i in range(3):\n",
        "    print(\"################################\")\n",
        "    print(f\"Report number: {i+1}\")\n",
        "    print(df.loc[i, 'Report Impression'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRjaMrntzgDR",
        "colab_type": "text"
      },
      "source": [
        "### Introducing BioC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qIaBkqSzgDR",
        "colab_type": "text"
      },
      "source": [
        "Let's get started by looking at the `BioC` module. You'll be using `BioC` to convert your clinical data into a standard format that can be leveraged on more specialized libraries. This module is used for many other NLP tasks as well, such as serialization or deserialization of data. You can read more about it [here](http://bioc.sourceforge.net/).\n",
        "\n",
        "For your purposes, you're interested in the `BioCCollection` object, which represents a collection of documents for a project. The collection might be an entire corpus, or a partial one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDS5e8zcz0fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install bioc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsXwEXP0zgDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bioc\n",
        "\n",
        "collection = bioc.BioCCollection()\n",
        "print(f\"attributes with value: \\n\\n{collection.__dict__}\\n\")\n",
        "print(f\"methods and attributes: \\n\\n{dir(collection)}\\n\")\n",
        "print(f\"documents within collection: {collection.documents}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H022ECOBzgDU",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the Text for BioC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlsh0KiOzgDV",
        "colab_type": "text"
      },
      "source": [
        "When working with collections, you're mostly interested in the documents attribute and the `add_document()` method.\n",
        "\n",
        "The `BioC` module gives you a standard format that allows you to apply other, more specialized libraries. Before seeing `BioC` in action, let's introduce `NegBio`, a tool that distinguishes negative or uncertain findings in radiology reports. It accomplishes this by using patterns on universal dependencies, instead of using rule-based methods. If you'd like to know more, check out the official github [repo](https://github.com/ncbi-nlp/NegBio), or the official [documentation](https://negbio.readthedocs.io/en/latest/index.html).\n",
        "\n",
        "You'll be using the `NegBioSSplitter` object to split your text into sentences. However, in order to do this, you'll first need to convert your text into a format that `BioC` supports. For this you'll use the `text2bioc()` function, which transforms the text into a `BioC` XML file. You can go even further and convert the text into documents with the `text2document()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVluTqx9z-_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install negbio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OZtsOeGzgDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from negbio.pipeline.ssplit import NegBioSSplitter\n",
        "from negbio.pipeline import text2bioc\n",
        "\n",
        "splitter = NegBioSSplitter()\n",
        "for i, report in enumerate(df[\"Report Impression\"]):\n",
        "        document = text2bioc.text2document(str(i), report)\n",
        "        document = splitter.split_doc(document)\n",
        "        collection.add_document(document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60apgVh5zgDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collection.documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jva-w_VzzgDb",
        "colab_type": "text"
      },
      "source": [
        "### Interpreting the Documents\n",
        "\n",
        "Now your `BioC` collection has been filled with documents, but the output is very hard to read. Let's break it down a little more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-Se17QqzgDb",
        "colab_type": "code",
        "outputId": "9afa7663-b61e-4b64-c2a2-11a71c4180d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(collection.documents)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0lEJYUXzgDe",
        "colab_type": "text"
      },
      "source": [
        "Looks like you have a document for each report impression. But what's stored inside each document? Let's check the first one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsHTMgBuzgDe",
        "colab_type": "code",
        "outputId": "e0088250-ae38-4f33-c17c-1920735027bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "collection.documents[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BioCDocument[id=0,infons=[],passages=[BioCPassage[offset=0,text='\\n \\n1.mild pulmona ... lation pads.  \\n \\n',infons=[],sentences=[BioCSentence[offset=0,text='\\n \\n1.mild pulmona ... and cardiomegaly.',infons=[],annotations=[],relations=[],],BioCSentence[offset=46,text='trace pleural fluid \\neffusions.',infons=[],annotations=[],relations=[],],BioCSentence[offset=80,text='2.low lung volume ... ilar atelectasis.',infons=[],annotations=[],relations=[],],BioCSentence[offset=135,text='3.no new focal consolidation.',infons=[],annotations=[],relations=[],],BioCSentence[offset=168,text='4.interval placem ... ibrillation pads.',infons=[],annotations=[],relations=[],]],annotations=[],relations=[],]],annotations=[],relations=[],]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkMotPcnzgDh",
        "colab_type": "text"
      },
      "source": [
        "Each document has an attribute called \"passages\" in which the sentences are stored. Notice that `passages` is a list, but for this case it will only have one element:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqyvCVkxzgDh",
        "colab_type": "code",
        "outputId": "5f2dd7b6-08d9-482e-b794-ca34f5fc0ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "collection.documents[0].passages[0].sentences"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[BioCSentence[offset=0,text='\\n \\n1.mild pulmona ... and cardiomegaly.',infons=[],annotations=[],relations=[],],\n",
              " BioCSentence[offset=46,text='trace pleural fluid \\neffusions.',infons=[],annotations=[],relations=[],],\n",
              " BioCSentence[offset=80,text='2.low lung volume ... ilar atelectasis.',infons=[],annotations=[],relations=[],],\n",
              " BioCSentence[offset=135,text='3.no new focal consolidation.',infons=[],annotations=[],relations=[],],\n",
              " BioCSentence[offset=168,text='4.interval placem ... ibrillation pads.',infons=[],annotations=[],relations=[],]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1v5-_lazgDj",
        "colab_type": "text"
      },
      "source": [
        "Each sentence stores information about the text, offset, relations and annotations. Let's check the sentences saved in the first document of our collection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hokCRhUSzgDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,s in enumerate(collection.documents[0].passages[0].sentences):\n",
        "    print(f\"sentence number {i + 1}: {s.text}\\n\")\n",
        "    print(\"###############################\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpmSYtqzgDm",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning up with the clean() function\n",
        "\n",
        "Notice how the first report impression, which had two sentences, was split successfully. However, the newlines have not been trimmed. The `clean()` function from the previous lecture notebook will come in handy here. Let's bring it back out of the toolbox and apply it in this notebook!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFUF1sc7zgDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean(sentence):\n",
        "    lower_sentence = sentence.lower()\n",
        "    corrected_sentence = re.sub('and/or', 'or', lower_sentence)\n",
        "    corrected_sentence = re.sub('(?<=[a-zA-Z])/(?=[a-zA-Z])', ' or ', corrected_sentence)\n",
        "    clean_sentence = corrected_sentence.replace(\"..\", \".\")\n",
        "    punctuation_spacer = str.maketrans({key: f\"{key} \" for key in \".,\"})\n",
        "    clean_sentence = clean_sentence.translate(punctuation_spacer)\n",
        "    clean_sentence = ' '.join(clean_sentence.split())\n",
        "    return clean_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4BmA7SezgDp",
        "colab_type": "text"
      },
      "source": [
        "### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYcuhSCTzgDp",
        "colab_type": "text"
      },
      "source": [
        "Now that you've spent some time exploring how the `NegBio` library works, let's try it out on your data. \n",
        "\n",
        "You'll determine whether a given report impression can tell you if a patient has an existing condition, while taking into account whether there was negation or uncertainty in the findings. For this task, you'll use these predetermined categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZcLUH9ezgDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORIES = [\"Cardiomegaly\", \"Lung Lesion\", \"Airspace Opacity\", \"Edema\", \n",
        "              \"Consolidation\", \"Pneumonia\", \"Atelectasis\", \"Pneumothorax\", \n",
        "              \"Pleural Effusion\", \"Pleural Other\", \"Fracture\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ISq_YazgDs",
        "colab_type": "text"
      },
      "source": [
        "### Import NegBio Dependencies\n",
        "\n",
        "Next you'll import everything you need for this task. Don't be alarmed by the declared paths below the imports! They're just mapping the path to various files that `NegBio` relies on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcmEBBW13Gkp",
        "colab_type": "text"
      },
      "source": [
        "Download the GENIA+PubMed parsing model:\n",
        "As a workaround, you could try downloading the compressed archive from that DropBox link yourself and extracting it into the ~/.local/share/bllipparser/GENIA+PubMed/ folder.\n",
        "\n",
        "(Model directory: /root/.local/share/bllipparser/GENIA+PubMed\n",
        "Fetching model: GENIA+PubMed from https://www.dropbox.com/s/ev3h78gq7526xdj/BLLIP-GENIA-PubMed.tar.bz2?dl=1\n",
        "Download complete                    \n",
        "Downloaded to temporary file /tmp/tmpyeo2jgku.bz2\n",
        "Extracting with 'tar xvjf /tmp/tmpyeo2jgku.bz2' to /root/.local/share/bllipparser/GENIA+PubMed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ON3ssvPpj3kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install bllipparser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562dVcJlkCYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "718ec0e0-77e8-441b-d127-e5aaa6fd178a"
      },
      "source": [
        "from bllipparser import RerankingParser\n",
        "rrp = RerankingParser.fetch_and_load('GENIA+PubMed', verbose=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model directory: /root/.local/share/bllipparser/GENIA+PubMed\n",
            "Fetching model: GENIA+PubMed from https://www.dropbox.com/s/ev3h78gq7526xdj/BLLIP-GENIA-PubMed.tar.bz2?dl=1\n",
            "Download complete                    \n",
            "Downloaded to temporary file /tmp/tmpyeo2jgku.bz2\n",
            "Extracting with 'tar xvjf /tmp/tmpyeo2jgku.bz2' to /root/.local/share/bllipparser/GENIA+PubMed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0xbHfBK3fi3",
        "colab_type": "text"
      },
      "source": [
        "NegBio is a high-performance NLP tool for negation and uncertainty detection in clinical texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obM1Zasuk_k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/ncbi-nlp/NegBio.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti2ZPpPpzgDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pathlib2 import Path\n",
        "from negbio.main_chexpert import pipeline\n",
        "from negbio.pipeline.parse import NegBioParser\n",
        "from negbio.chexpert.stages.load import NegBioLoader\n",
        "from negbio.chexpert.stages.extract import NegBioExtractor\n",
        "from negbio.chexpert.stages.classify import ModifiedDetector\n",
        "from negbio.chexpert.stages.aggregate import NegBioAggregator\n",
        "from negbio.pipeline.ptb2ud import NegBioPtb2DepConverter, Lemmatizer\n",
        "\n",
        "PARSING_MODEL_DIR = \"~/.local/share/bllipparser/GENIA+PubMed\"\n",
        "CHEXPERT_PATH = \"NegBio/negbio/chexpert/\"\n",
        "MENTION_PATH =f\"{CHEXPERT_PATH}phrases/mention\"\n",
        "UNMENTION_PATH = f\"{CHEXPERT_PATH}phrases/\"\n",
        "NEG_PATH = f'{CHEXPERT_PATH}patterns/negation.txt'\n",
        "PRE_NEG_PATH = f'{CHEXPERT_PATH}patterns/pre_negation_uncertainty.txt'\n",
        "POST_NEG_PATH = f'{CHEXPERT_PATH}patterns/post_negation_uncertainty.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9U5VJNizgDv",
        "colab_type": "text"
      },
      "source": [
        "The encoding of information within these files is beyond the scope of this notebook, but if you're really curious about the contents you could do something like this to see more: \n",
        "```python\n",
        "!cat $NEG_PATH\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upWRKg5jzgDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat $NEG_PATH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dRnyOkCzgDx",
        "colab_type": "text"
      },
      "source": [
        "Running this process for the entire dataset is very slow (~1.5 hr on a fast laptop!) so let's slice it to showcase how `NegBio` works. Let's start with 50 random observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bCUJVZjzgDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_df = df.sample(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3g53eJyzgD0",
        "colab_type": "text"
      },
      "source": [
        "Also, let's recreate the code from the beginning of the notebook as a function, including the `clean()` function as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5P2gcO_zgD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_bioc_collection(df):\n",
        "    collection = bioc.BioCCollection()\n",
        "    splitter = NegBioSSplitter()\n",
        "    for i, report in enumerate(df[\"Report Impression\"]):\n",
        "        document = text2bioc.text2document(str(i), clean(report))\n",
        "        document = splitter.split_doc(document)\n",
        "        collection.add_document(document)\n",
        "    return collection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0O_2_V-zgD3",
        "colab_type": "text"
      },
      "source": [
        "Here, you'll repeat your process from earlier by converting the report impression strings into a `BioC` XML format which `NegBio` can read."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-yOVo7NzgD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "collection = get_bioc_collection(sampled_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9E8l6-gzgD5",
        "colab_type": "text"
      },
      "source": [
        "Now let's instantiate `NegBio`'s lemmatizer. \n",
        "\n",
        "The process of lemmatization refers to returning the dictionary form of a word (or lemma) by removing inflectional endings. It's very cool and you can read more about it [here](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL9Q8OnQzgD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = Lemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B82UB6IzgD7",
        "colab_type": "text"
      },
      "source": [
        "Next you'll instantiate `NegBio`'s converter to convert from parse tree to universal dependencies. This is done using the Stanford converter, which you can find more information about [here](https://github.com/dmcc/PyStanfordDependencies).\n",
        "\n",
        "The parse tree used here is the [Penn Treebank](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html). In general terms, a treebank is an annotated text corpus that includes analysis beyond part-of-speech tagging. They've become very valuable resources to NLP research in recent years.\n",
        "\n",
        "Universal dependencies, or UD, provide a powerful framework for annotating grammar across different languages. Read more about them [here](https://universaldependencies.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7wATzfczgD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ptb2dep = NegBioPtb2DepConverter(lemmatizer, universal=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rr55eXtzgD_",
        "colab_type": "text"
      },
      "source": [
        "You've already seen the splitter in action before, so you can skip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqyWA5i2zgD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ssplitter = NegBioSSplitter(newline=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBJntnqbzgEB",
        "colab_type": "text"
      },
      "source": [
        "Now you'll instantiate the parser and the loader. \n",
        "\n",
        "Under the hood, you're using the [BLIPP reranking parser](https://github.com/BLLIP/bllip-parser), which is a statistical natural language parser. \n",
        "\n",
        "The loader, as you might imagine, loads the reports into memory.\n",
        "\n",
        "Over all of this, the [chexpert-labeler](https://github.com/stanfordmlgroup/chexpert-labeler) is used. This labeler extracts observations from radiology reports specifically, and can provide a vocabulary appropriate to the clinical context. \n",
        "\n",
        "https://github.com/alistairewj/chexpert-labeler\n",
        "\n",
        "NOTE: not working right now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNoW7fVbzgEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parser = NegBioParser(model_dir=PARSING_MODEL_DIR)\n",
        "loader = NegBioLoader()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj4OXfWezgED",
        "colab_type": "text"
      },
      "source": [
        "The extractor is what extracts the observations from the report impressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0lCv_8OzgED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "extractor = NegBioExtractor(Path(MENTION_PATH), Path(UNMENTION_PATH))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBuIu9OhzgEG",
        "colab_type": "text"
      },
      "source": [
        "The negator will determine whether negation or uncertainty exists in the context of the observations provided by the extractor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUt3_EGgzgEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neg_detector = ModifiedDetector(PRE_NEG_PATH, NEG_PATH, POST_NEG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k49SvZTqzgEI",
        "colab_type": "text"
      },
      "source": [
        "The aggregator then aggregates these observations if they belong to the same category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC5vOgBkzgEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggregator = NegBioAggregator(CATEGORIES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16l9SpXNzgEK",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw6gTvrYzgEL",
        "colab_type": "text"
      },
      "source": [
        "Finally, you'll put everything together using the pipeline function, which takes as arguments all of the objects you've instantiated so far. Then you'll get a nice, clean DataFrame with your result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__OquBRxzgEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "6b683834-0c9c-4163-ec0b-badf508a21ff"
      },
      "source": [
        "collection = pipeline(collection, loader, ssplitter, extractor, \n",
        "                          parser, ptb2dep, neg_detector, aggregator, verbose=True)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/50 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-903b2238d4fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m collection = pipeline(collection, loader, ssplitter, extractor, \n\u001b[0;32m----> 2\u001b[0;31m                           parser, ptb2dep, neg_detector, aggregator, verbose=True)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/negbio/main_chexpert.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(collection, loader, ssplitter, extractor, parser, ptb2dep, neg_detector, aggregator, verbose)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mptb2dep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegdetect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_detector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/negbio/pipeline/parse.py\u001b[0m in \u001b[0;36mparse_doc\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpassage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse tree'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/negbio/pipeline/parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot parse empty sentence: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnbest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnbest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptb_parse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bllipparser/RerankingParser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, sentence, rerank, sentence_id)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mloaded\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mno\u001b[0m \u001b[0mparses\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0man\u001b[0m \u001b[0merror\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         this will return an empty NBestList.\"\"\"\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mrerank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_models_loaded_or_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrerank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bllipparser/RerankingParser.py\u001b[0m in \u001b[0;36mcheck_models_loaded_or_error\u001b[0;34m(self, rerank)\u001b[0m\n\u001b[1;32m    784\u001b[0m         rerank='auto').\"\"\"\n\u001b[1;32m    785\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parser_model_loaded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parser model has not been loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrerank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreranker_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reranker model has not been loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Parser model has not been loaded."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyfe5VJvzgEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negbio_pred = pd.DataFrame()\n",
        "for doc in collection.documents:\n",
        "    dictionary = {}\n",
        "    for key, val in doc.infons.items():\n",
        "        dictionary[key[9:]] = val\n",
        "    negbio_pred = negbio_pred.append(dictionary, ignore_index=True)\n",
        "negbio_pred = negbio_pred.replace(\n",
        "    \"Positive\", True).replace(\n",
        "    \"Negative\", False).replace(\"Uncertain\", False).fillna(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7BZfgMLzgEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negbio_pred.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJmmdPaVzgEQ",
        "colab_type": "text"
      },
      "source": [
        "Now you can check every entry in the report impressions for the presence of a condition, while knowing that negation has been taken into account. Really cool!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaNhQKrfzgER",
        "colab_type": "text"
      },
      "source": [
        "**Congratulations on finishing this notebook!!!** This was a very high-level explanation of everything that NegBio does and as you may have noticed, this library leverages many other great tools and libraries. Hopefully, it was a good introduction to how it works. **Nice work, keep it up!**"
      ]
    }
  ]
}